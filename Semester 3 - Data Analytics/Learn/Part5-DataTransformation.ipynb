{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a7766df",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span dir=\"rtl\" style=\"font-family:B Nazanin\" align=\"right\">\n",
    "    <h1>فصل سوم: تجزیه و تحلیل داده‌ها</h1>\n",
    "    <h2>بخش پنجم: تبدیل یا انتقال داده‌ها</h2>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac297db9",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<p style=\"font-family:B Nazanin\">\n",
    "    تا اینجای کار تلاش شد که داده‌ها پاکسازی شوند. سپس داده‌ها از منابع مختلف یکپارچه گردند و با یکدیگر تجمیع و ترکیب شوند و تا حد امکان آن‌ها را از ناسازگاری دور کنیم. سپس تلاش کردیم تا عمل کاهش داده‌ها را انجام دهیم تا پردازش داده‌ها سریع‌تر و با کیفیت و دقت بالاتری انجام پذیرد. اکنون لازم است تا داده‌های خام اولیه را به فرمت‌هایی تبدیل کنیم که الگوریتم‌های یادگیری ماشین، آن‌ها را بپذیرند و بتوانند با آن‌ها کار کنند. چون این داده‌ها برای پردازش‌های آینده ما مناسب نیستند. این تبدیل کردن داده‌ها (یا انتقال داده‌ها) هم روی قالب و ساختار داده‌ها انجام می‌پذیرد و هم روی مقدار داده‌ها. پس به هر تلاشی که داده‌های خام را به داده‌های مناسب تبدیل کند چه از نظر ساختاری و چه از نظر مقداری، تبدیل یا انتقال داده گفته می‌شود. البته می‌توان گفت بخش عمده‌ای از فرایند انتقال داده تا کنون انجام شده‌است. مثلا حذف نویز داده‌ها، از بین بردن مقایدر NaN، کاهش ابعاد داده‌ها و تشکیل ویژگی‌های جدید، خود بخشی از فرایند انتقال داده‌هاست. اما انتقال داده‌ها شامل فرایندهای دیگری نیز هست که در اینجا توضیح داده می‌شوند. <br>\n",
    "    <br>\n",
    "    <b>فعالیت‌های فرایند انتقال داده‌ها:</b><br>\n",
    "    &nbsp;&nbsp;&nbsp; * هموارسازی داده‌ها (Data Smoothing): وقتی داده‌های پرت را کنار می‌گذاریم و نویزها را از بین می‌بریم، به نوعی هموارسازی داده‌ها را تا حدودی انجام داده‌ایم. اما گاهی داده‌های پیچیده‌ای وجود دارد که داده‌های درستی هستند. نه نویز هستند و نه داده پرت و این داده‌ها را صرفا برای کاهش پیچیدگی کنار می‌گذاریم. پس داده ناشناخته‌ای که درست است اما اگر در دیتاست باقی بماند، پیچیدگی کار را بالا می‌برد نیز باید حذف شود که به این عمل هموارسازی داده می‌گویند. مثلا خریدهای معمول یک کاربر در یک سایت در حد یکی دو میلیون است، ولی یک بار یک خرید چند صد میلیونی انجام می‌دهد. <br>\n",
    "    &nbsp;&nbsp;&nbsp; * استخراج یا ساخت ویژگی (Feature Extraction or Cinstruction): با ترکیب چند ویژگی یک ویژگی جدید را تولید کنیم. <br>\n",
    "    &nbsp;&nbsp;&nbsp; * تجمیع داده‌ها (Data Aggregation): گاهی به داده‌های جزئی‌تر نیازی نداریم و می‌توانیم با تجمیع داده‌ها، داده‌های کلی‌تری را تولید کنیم. مثلا آمار فروش یک سایت به صورت ساعتی موجود است ولی ما در تجزیه و تحلیل آمار روزانه را لازم داریم. پس با تجمیع داده‌های ساعتی،‌می‌توان آمار روزانه را به‌دست آورد. <br>\n",
    "    &nbsp;&nbsp;&nbsp; * مقیاس‌بندی داده‌ها (Data Scaling): این فرایند تلاش دارد که تا حد امکان فاصله میان داده‌های یک دیتاست را کم کند تا داده‌ها به هم نزدیک شوند. به عنوان مثال فرض کنید که دیتاستی از اطلاعات چند کارمند داریم که بازه سنی آن‌ها بین 10 تا 90 سال است. قد افراد در بازه 100 تا 200 است. وزن عددی بین 20 تا 150 است. همه این اعداد تقریبا در یک مقیاس هستند. مثلا همگی عددی بین 0 تا 200 هستند. اما در کنار این ویژگی‌ها، ویژگی حقوق افراد نیز ثبت می‌شود که مقیاس آن در حدود چند میلیون است. این اتفاق باعث می‌شود که در آینده در الگوریتم‌های یادگیری ماشین و هوش مصنوعی به نتیجه درستی نرسیم و نتیجه تجزیه و تحلیل را به هم می‌ریزد. پس در فرایند انتقال داده‌ها تلاش می‌شود مقیاس داده‌ها طوری انتخاب شود که همگی داده‌ها تا حد امکان نزدیک به هم شوند. <br>\n",
    "    &nbsp;&nbsp;&nbsp; * کدگذاری ویژگی‌ها (Data Encoding): معمولا برای داده‌های غیر عددی استفاده می‌شود تا داده‌های غیرعددی را به داده‌های عددی تبدیل کنیم تا الگوریتم‌ها بتوانند روی آن‌ها کار کنند. کدگذاری داده‌ها هم باعث کاهش داده‌ها می‌شوند و هم به نوعی عمل انتقال داده را انجام می‌دهند. دو روش متداول برای این کار One Hot Encoding و Label Encoding است.<br>\n",
    "    به مجموعه فعالیت‌هایی که تا اینجای کار توضیح داده‌شد تا داده خام اولیه را به داده‌های مناسب برای الگوریتم‌ها آماده کنیم، همان فرایند ETL گفته می‌شود. <br>\n",
    "    <br>\n",
    "    <br>\n",
    "    <b>استخراج و تولید ویژگی:</b><br>\n",
    "    برای این فرایند چند روش وجود دارد: <br>\n",
    "    &nbsp;&nbsp;&nbsp; * انتخاب ویژگی (Feature Selection): یعنی از بین ویژگی‌های موجود، یک یا چند ویژگی را انتخاب کنیم و سایر ویژگی‌ها را حذف کنیم. <br>\n",
    "    &nbsp;&nbsp;&nbsp; * استخراج ویژگی (Feature Extraction): یعنی با استفاده از ترکیب و ادغام چند ویژگی، ویژگی جدیدی را ایجاد کنیم که معرف ویژگی‌های قبلی باشد. <br>\n",
    "    &nbsp;&nbsp;&nbsp; * تولید ویژگی (featuer Generation): تولید یا اختراع ویژگی یعنی ویژگی‌هایی را اضافه کنیم که فقط به تحلیل ما کمک کنند. تفاوت این روش با استخراج این است که در روش استخراج ما چند ویژگی را با هم ترکیب می‌کنیم تا داده‌ها را کم کنیم. اما در تولید ویژگی داده‌ها را زیاد می‌کنیم. <br>\n",
    "    &nbsp;&nbsp;&nbsp; * ارزیابی ویژگی (Feature Evaluation): یعنی ویژگی‌ها را ارزیابی کنیم و آن‌ها را از نظر کم‌اهمیت یا پراهمیت‌بودن دسته بندی کنیم که این کار می‌تواند به فرایندهای استخراج و انتخاب ویژگی نیز کمک کند. <br>\n",
    "    <br>\n",
    "    <br>\n",
    "    <b>مقیاس‌بندی داده‌ها (Data Scaling):</b><br>\n",
    "    برای سادگی تجزیه و تحلیل و همچنین برای این که الگوریتم‌ها درست‌تر کار کنند، نیاز داریم که داده‌ها را به هم نزدیک کنیم و مقیاس آن‌ها به هم نزدیک باشد. به طور کلی هر روشی که بتواند داده‌ها را به هم نزدیک کند برای این فرایند جواب می‌دهد ولی روش‌های استاندارد مقیاس‌بندی به شرح زیر است: <br>\n",
    "    &nbsp;&nbsp;&nbsp; * نرمال‌سازی (Normalization): با استفاده از این روش می‌توان داده‌های عددی را به عددی در بازه صفر تا یک تبدیل کنیم. برای این کار مینیمم و ماکزیمم داده‌های یک ستون را پیدا می‌کنیم و با استفاده از فرمول زیر داده جدید را پیدا می‌کنیم: \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d75071c",
   "metadata": {},
   "source": [
    "$$\n",
    "x_{norm} = \\frac{x - min(x)}{max(x) - min(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d1a6f3",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<p style=\"font-family:B Nazanin\">\n",
    "    &nbsp;&nbsp;&nbsp; * استانداردسازی (Standardization): در این روش با پیداکردن میانگین و انحراف معیار یک ستون، با فرمول زیر داده را به مقدار جدیدی تبدیل می‌کنیم که به آن نرمال‌سازی آماری نیز می‌گویند. این روش نیز عددی بین صفر و یک تولید می‌کند. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8e2c80",
   "metadata": {},
   "source": [
    "$$\n",
    "x_{stand} = \\frac{x - mean(x)}{standard\\ deviation(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1b9afe",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<p style = \"font-family:B Nazanin;\">\n",
    "    \n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
